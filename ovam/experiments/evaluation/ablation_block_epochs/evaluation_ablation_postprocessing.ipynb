{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(annotation, mask):\n",
    "    \"\"\"Annotation: ground truth (512, 512), Mask: prediction (512, 512)\"\"\"\n",
    "\n",
    "    # Compute intersection\n",
    "    intersection = np.sum(np.logical_and(annotation, mask))\n",
    "\n",
    "    # Compute union\n",
    "    union = np.sum(np.logical_or(annotation, mask))\n",
    "\n",
    "    # Compute intersection over union\n",
    "    iou_score = intersection / union\n",
    "\n",
    "    return intersection, union, iou_score\n",
    "\n",
    "\n",
    "def interpolate(heatmap: np.ndarray, size=(512, 512), mode=\"bilinear\"):\n",
    "    \"\"\"Interpolate heatmap to match the size of the ground truth\"\"\"\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    heatmap = torch.from_numpy(heatmap)\n",
    "    # Add batch and channel dimension\n",
    "    heatmap = heatmap.unsqueeze(0)\n",
    "    if heatmap.ndim != 4:\n",
    "        heatmap = heatmap.unsqueeze(0)\n",
    "    # Interpolate\n",
    "    heatmap = torch.nn.functional.interpolate(heatmap, size=size, mode=mode)\n",
    "    # Convert back to numpy\n",
    "    heatmap = heatmap.squeeze().squeeze().numpy()\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import denseCRF\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "def densecrf(I, P):\n",
    "    \"\"\"\n",
    "    input parameters:\n",
    "        I    : a numpy array of shape [H, W, C], where C should be 3.\n",
    "               type of I should be np.uint8, and the values are in [0, 255]\n",
    "        P    : a probability map of shape [H, W, L], where L is the number of classes\n",
    "               type of P should be np.float32\n",
    "        param: a tuple giving parameters of CRF (w1, alpha, beta, w2, gamma, it), where\n",
    "                w1    :   weight of bilateral term, e.g. 10.0\n",
    "                alpha :   spatial distance std, e.g., 80\n",
    "                beta  :   rgb value std, e.g., 15\n",
    "                w2    :   weight of spatial term, e.g., 3.0\n",
    "                gamma :   spatial distance std for spatial term, e.g., 3\n",
    "                it    :   iteration number, e.g., 5\n",
    "    output parameters:\n",
    "        out  : a numpy array of shape [H, W], where pixel values represent class indices. \n",
    "    \"\"\"\n",
    "    w1    = 10.0  # weight of bilateral term\n",
    "    alpha = 80    # spatial std\n",
    "    beta  = 13    # rgb  std\n",
    "    w2    = 3.0   # weight of spatial term\n",
    "    gamma = 3     # spatial std\n",
    "    it    = 5.0   # iteration\n",
    "    param = (w1, alpha, beta, w2, gamma, it)\n",
    "    out = denseCRF.densecrf(I, P, param) \n",
    "    return out   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC-sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('voc_sim')\n",
    "\n",
    "annotations_folder = Path('../ovam/voc_sim/annotations')\n",
    "images_folder = Path('../ovam/voc_sim/images')\n",
    "sa_folder =  Path('../ovam_sa/voc_sim/sa')\n",
    "\n",
    "\n",
    "def load_heatmap(classname, seed, res16, res32, res64, opt):\n",
    "        hs = []\n",
    "        opt_suffix = \"\" if not opt else \"_opt\"\n",
    "        if res16:\n",
    "            filename = dataset_path / f\"{classname}_{seed.replace('seed', '')}_a16{opt_suffix}.npy\"\n",
    "            h = np.load(filename)\n",
    "            # INterpolate to 64x64\n",
    "            h = interpolate(h, size=(64, 64))\n",
    "            hs.append(h)\n",
    "        if res32:\n",
    "            filename = dataset_path / f\"{classname}_{seed.replace('seed', '')}_a32{opt_suffix}.npy\"\n",
    "            h = np.load(filename)\n",
    "            # INterpolate to 64x64\n",
    "            h = interpolate(h, size=(64, 64))\n",
    "            hs.append(h)\n",
    "        if res64:\n",
    "            filename = dataset_path / f\"{classname}_{seed.replace('seed', '')}_a64{opt_suffix}.npy\"\n",
    "            h = np.load(filename)\n",
    "            hs.append(h)\n",
    "        heatmap = np.stack(hs, axis=0)\n",
    "        heatmap = heatmap.sum(axis=0)\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "\n",
    "apply_crf = True\n",
    "suffix_crf = \"_crf\" if apply_crf else \"\"\n",
    "min_sa = 0.85\n",
    "min_sa_optimized = 0.95\n",
    "\n",
    "re_results = []\n",
    "res16, res32, res64 = True, True, True\n",
    "# Eight combinations of res16, res32, res64\n",
    "for apply_sa, apply_crf in [(False, False), (True, False), (False, True), (True, True)]:\n",
    "    if apply_sa:\n",
    "        # Hyperparameters without optimization\n",
    "        threshold = 0.4 # Threshold used in the paper DAAM\n",
    "        threshold_optimized = 0.75\n",
    "    else:\n",
    "        threshold = 0.4 # Threshold used in the paper DAAM\n",
    "        threshold_optimized = 0.8\n",
    "\n",
    "    # Iterate throuth annotations\n",
    "    results = []\n",
    "    \n",
    "    for annotation_path in tqdm(list(annotations_folder.iterdir())):\n",
    "        example_result_dict = {}\n",
    "        classname, model, seed, _ = annotation_path.stem.split('_')\n",
    "        image_path = images_folder / f\"{classname}_{model}_{seed}.png\"\n",
    "        sa_path = sa_folder / f\"sa15_{classname}_{seed.replace('seed', '')}_sa.npy\"\n",
    "        heatmap = load_heatmap(classname, seed, res16, res32, res64, opt=False)\n",
    "        heatmap_optimized = load_heatmap(classname, seed, res16, res32, res64, opt=True)\n",
    "        \n",
    "        # Check all exists\n",
    "        assert annotation_path.exists(), f\"Annotation {annotation_path} does not exist\"\n",
    "        assert image_path.exists(), f\"Image {image_path} does not exist\"\n",
    "        assert sa_path.exists(), f\"SA {sa_path} does not exist\"\n",
    "        assert heatmap.shape[1:] == (64, 64)\n",
    "        assert heatmap.shape[0] < 10\n",
    "        assert heatmap_optimized.shape[1:] == (64, 64)\n",
    "        assert heatmap_optimized.shape[0] == 2\n",
    "\n",
    "        \n",
    "        example_result_dict['classname'] = classname\n",
    "        example_result_dict['model'] = model\n",
    "        example_result_dict['seed'] = seed\n",
    "        example_result_dict['image_path'] = image_path.name\n",
    "        example_result_dict['annotation_path'] = annotation_path.name\n",
    "        #example_result_dict['heatmap_path'] = heatmap_path.name\n",
    "        #example_result_dict['heatmap_optimized_path'] = heatmap_optimized_path.name\n",
    "        \n",
    "\n",
    "        # Load image\n",
    "        image = np.array(Image.open(image_path))\n",
    "        assert image.shape == (512, 512, 3), f\"Image {image_path} has wrong shape {image.shape}\"\n",
    "        \n",
    "        # Load annotation. Convert in binary mask\n",
    "        annotation = np.array(Image.open(annotation_path))\n",
    "        assert annotation.shape == (512, 512, 3), f\"Annotation {annotation_path} has wrong shape {annotation.shape}\"\n",
    "        annotation = annotation.sum(axis=-1) != 0\n",
    "        assert annotation.shape == (512, 512), f\"Annotation aggregated {annotation_path} has wrong shape {annotation.shape}\"\n",
    "        \n",
    "        \n",
    "        # Load Self-Attention map\n",
    "        sa = np.load(sa_path)\n",
    "        assert sa.shape == (64, 64)\n",
    "        sa = interpolate(sa, size=(512, 512))\n",
    "        assert sa.shape == (512, 512)\n",
    "\n",
    "\n",
    "        # Reescale self-attention to [min_sa, 1]\n",
    "        sa = (sa - sa.min()) / (sa.max() - sa.min())\n",
    "        sa = sa * (1 - min_sa) + min_sa\n",
    "        assert abs(sa.min()- min_sa) < 0.001, f\"Min sa {sa.min()} is not {min_sa}\"\n",
    "        assert abs(sa.max()- 1) < 0.001, f\"Max sa {sa.max()} is not 1\"\n",
    "        \n",
    "        # Binarize using DAAM procedure\n",
    "        heatmap = interpolate(heatmap[-2], size=(512, 512))\n",
    "        assert heatmap.shape == (512, 512), f\"Heatmap {heatmap_path} has wrong shape {heatmap.shape}\"\n",
    "        heatmap = heatmap / heatmap.max()\n",
    "        if apply_sa:\n",
    "            heatmap = heatmap * sa # Apply sa\n",
    "        mask = heatmap > threshold\n",
    "\n",
    "        if apply_crf:\n",
    "            mask = np.stack([1 - mask, mask], axis=-1).astype(np.float32)\n",
    "            mask = densecrf(image, mask)\n",
    "\n",
    "        assert mask.shape == (512, 512), f\"Mask {mask} has wrong shape {mask.shape}\"\n",
    "\n",
    "        i_normal, u_normal, iou_normal = compute_iou(annotation=annotation, mask=mask)\n",
    "        example_result_dict['iou_normal'] = iou_normal\n",
    "        example_result_dict['i_normal'] = i_normal\n",
    "        example_result_dict['u_normal'] = u_normal\n",
    "\n",
    "\n",
    "        \n",
    "        # Load mask (optimized)\n",
    "        heatmap_optimized = heatmap_optimized[1] # We stored in 0 the background and in 1 the token related to the foreground object\n",
    "        assert heatmap_optimized.shape == (64, 64), f\"Heatmap {heatmap_optimized_path} has wrong shape {heatmap_optimized.shape}\"\n",
    "        heatmap_optimized = interpolate(heatmap_optimized, size=(512, 512))\n",
    "        assert heatmap_optimized.shape == (512, 512), f\"Heatmap {heatmap_optimized_path} has wrong shape {heatmap_optimized.shape}\"\n",
    "\n",
    "\n",
    "        # Reescale self-attention to [min_sa_optimized, 1]\n",
    "        sa = (sa - sa.min()) / (sa.max() - sa.min())\n",
    "        sa = sa * (1 - min_sa_optimized) + min_sa_optimized\n",
    "        assert abs(sa.min()- min_sa_optimized) < 0.001, f\"Min sa {sa.min()} is not {min_sa}\"\n",
    "        assert abs(sa.max()- 1) < 0.001, f\"Max sa {sa.max()} is not 1\"\n",
    "\n",
    "        # Binarize using DAAM procedure\n",
    "        heatmap_optimized = heatmap_optimized / heatmap_optimized.max()\n",
    "        if apply_sa:\n",
    "            heatmap_optimized = heatmap_optimized * sa\n",
    "        mask_optimized = heatmap_optimized > threshold_optimized\n",
    "        if apply_crf:\n",
    "            mask_optimized = np.stack([1 - mask_optimized, mask_optimized], axis=-1).astype(np.float32)\n",
    "            mask_optimized = densecrf(image, mask_optimized)\n",
    "\n",
    "        assert mask_optimized.shape == (512, 512), f\"Mask {mask_optimized} has wrong shape {mask_optimized.shape}\"\n",
    "\n",
    "        i_optimized, u_optimized, iou_optimized = compute_iou(annotation=annotation, mask=mask_optimized)\n",
    "        example_result_dict['iou_optimized'] = iou_optimized\n",
    "        example_result_dict['i_optimized'] = i_optimized\n",
    "        example_result_dict['u_optimized'] = u_optimized\n",
    "\n",
    "        results.append(example_result_dict)\n",
    "        \n",
    "        \n",
    "    # Aggregated by example\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['experiment'] = \"voc-sim - daam\"\n",
    "    # df_results.to_csv(dataset_path / f'ovam_voc_sim_results_sa{suffix_crf}.csv', index=False)\n",
    "\n",
    "\n",
    "    # Aggregated results by class\n",
    "    df_classes = df_results.groupby(['classname', 'model']).aggregate({'i_normal': 'sum', 'u_normal': 'sum', 'i_optimized': 'sum', 'u_optimized': 'sum'}).reset_index()\n",
    "    df_classes['iou_normal'] = df_classes['i_normal'] / df_classes['u_normal']\n",
    "    df_classes['iou_optimized'] = df_classes['i_optimized'] / df_classes['u_optimized']\n",
    "    df_classes['experiment'] = \"voc-sim - ovam\"\n",
    "    df_classes = df_classes.sort_values('classname').reset_index(drop=True)\n",
    "    # df_classes.to_csv(dataset_path / f'ovam_voc_sim_class_results_sa{suffix_crf}.csv', index=False)\n",
    "\n",
    "    df_overall = df_classes.groupby('model').aggregate({'i_normal': 'sum', 'u_normal': 'sum', 'i_optimized': 'sum', 'u_optimized': 'sum',\n",
    "                                            'iou_normal': 'mean', 'iou_optimized': 'mean'}).reset_index()\n",
    "\n",
    "    df_overall.rename(columns={'iou_normal': 'miou_normal', 'iou_optimized': 'miou_optimized'}, inplace=True)\n",
    "    df_overall['iou_overall_normal'] = df_overall['i_normal'] / df_overall['u_normal']\n",
    "    df_overall['iou_overall_optimized'] = df_overall['i_optimized'] / df_overall['u_optimized']\n",
    "    df_overall['experiment'] = \"voc-sim - ovam\"\n",
    "\n",
    "    #df_overall.to_csv(dataset_path / f'ovam_voc_sim_overall_results_sa{suffix_crf}.csv', index=False)\n",
    "\n",
    "    df_overall_display = df_overall[[\"miou_normal\",\"iou_overall_normal\", \"miou_optimized\",  \"iou_overall_optimized\"]]\n",
    "    df_overall_display = (100*df_overall_display).round(1)\n",
    "    df_overall_display[\"dcrf\"] = apply_crf\n",
    "    df_overall_display[\"sa\"] = apply_sa\n",
    "\n",
    "    display(df_overall_display)\n",
    "\n",
    "    re_results.append(df_overall_display)\n",
    "\n",
    "df_reresults = pd.concat(re_results)\n",
    "df_reresults\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO-cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path('coco_cap')\n",
    "\n",
    "annotations_folder = Path('../ovam/coco_captions/annotations')\n",
    "images_folder = Path('../ovam/coco_captions/images')\n",
    "sa_folder =  Path('../ovam_sa/coco_cap/sa')\n",
    "\n",
    "\n",
    "def load_heatmap(classname, seed,  caption, res16, res32, res64, opt):\n",
    "        hs = []\n",
    "        opt_suffix = \"\" if not opt else \"_opt\"\n",
    "        if res16:\n",
    "            filename = dataset_path / f\"{classname.replace(' ', '-')}_{caption}_{seed.replace('seed', '')}_a16{opt_suffix}.npy\"\n",
    "            h = np.load(filename)\n",
    "            # INterpolate to 64x64\n",
    "            h = interpolate(h, size=(64, 64))\n",
    "            hs.append(h)\n",
    "        if res32:\n",
    "            filename = dataset_path / f\"{classname.replace(' ', '-')}_{caption}_{seed.replace('seed', '')}_a32{opt_suffix}.npy\"\n",
    "            h = np.load(filename)\n",
    "            # INterpolate to 64x64\n",
    "            h = interpolate(h, size=(64, 64))\n",
    "            hs.append(h)\n",
    "        if res64:\n",
    "            filename = dataset_path / f\"{classname.replace(' ', '-')}_{caption}_{seed.replace('seed', '')}_a64{opt_suffix}.npy\"\n",
    "            h = np.load(filename)\n",
    "            hs.append(h)\n",
    "        heatmap = np.stack(hs, axis=0)\n",
    "        heatmap = heatmap.sum(axis=0)\n",
    "        return heatmap\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "# Hyperparameters without optimization\n",
    "threshold = .4 #0.5 # Threshold used in the paper DAAM\n",
    "min_sa = .75 # 0.91\n",
    "\n",
    "# Threshold found empirically using the training set employed for optimizng the tokens\n",
    "threshold_optimized = 0.8\n",
    "min_sa_optimized = .75 #0.99\n",
    "\n",
    "\n",
    "apply_crf = True\n",
    "suffix_crf = \"_crf\" if apply_crf else \"\"\n",
    "\n",
    "re_results = []\n",
    "heatmap_path = None\n",
    "heatmap_optimized_path = None\n",
    "\n",
    "res16, res32, res64 = True, True, True\n",
    "# Eight combinations of res16, res32, res64\n",
    "\n",
    "min_sa = 0.85\n",
    "min_sa_optimized = 0.95\n",
    "\n",
    "for apply_sa, apply_crf in [(False, False), (True, False), (False, True), (True, True)]:\n",
    "    if apply_sa:\n",
    "        # Hyperparameters without optimization\n",
    "        threshold = 0.4 # Threshold used in the paper DAAM\n",
    "        threshold_optimized = 0.8\n",
    "    else:\n",
    "        threshold = 0.4 # Threshold used in the paper DAAM\n",
    "        threshold_optimized = 0.75\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate throuth annotations\n",
    "    results = []\n",
    "    for annotation_path in tqdm(list(annotations_folder.iterdir())):\n",
    "        example_result_dict = {}\n",
    "        classname, model, _, caption, _, seed = annotation_path.stem.split('_')\n",
    "        model = model.replace('-', '')\n",
    "        caption = f\"caption{caption}\"\n",
    "        seed = f\"seed{seed}\"\n",
    "        \n",
    "        image_path = images_folder / f\"{classname}_{model}_{caption}_{seed}.png\"\n",
    "        sa_path = sa_folder / f\"sd15attn2mask_{classname.replace(' ', '-')}_{caption.replace('caption', '')}_{seed.replace('seed', '')}_sa.npy\"\n",
    "\n",
    "\n",
    "        heatmap = load_heatmap(classname, seed, caption, res16, res32, res64, opt=False)\n",
    "        heatmap_optimized = load_heatmap(classname, seed, caption, res16, res32, res64, opt=True)\n",
    "        #print(heatmap.shape)\n",
    "        assert annotation_path.exists(), f\"Annotation {annotation_path} does not exist\"\n",
    "        assert image_path.exists(), f\"Image {image_path} does not exist\"\n",
    "        assert sa_path.exists(), f\"Mask {sa_path} does not exist\"\n",
    "        \n",
    "\n",
    "        # Add paths to result dict\n",
    "        example_result_dict['classname'] = classname\n",
    "        example_result_dict['model'] = model\n",
    "        example_result_dict['seed'] = seed\n",
    "        \n",
    "        example_result_dict['image_path'] = image_path.name\n",
    "        example_result_dict['annotation_path'] = annotation_path.name\n",
    "        #example_result_dict['heatmap_path'] = heatmap_path.name\n",
    "        #example_result_dict['heatmap_optimized_path'] = heatmap_optimized_path.name\n",
    "\n",
    "        # Get info of coco caption used using caption_id\n",
    "        caption_id = int(caption.replace('caption', ''))\n",
    "        # row = df_coco_captions.query(\"caption_id==@caption_id\")\n",
    "        # assert len(row) == 1, f\"Caption {caption_id} not found in df_coco_captions\"\n",
    "        # row = row.iloc[0]\n",
    "        # prompt = row['caption']\n",
    "        # word_included = row['word_included']\n",
    "        # coco_categories = row['categories']\n",
    "\n",
    "\n",
    "        # Add info to results\n",
    "        example_result_dict['coco_caption_id'] = caption_id\n",
    "        #example_result_dict['prompt'] = prompt\n",
    "        #example_result_dict['word_included'] = word_included\n",
    "        #example_result_dict['coco_categories'] = coco_categories\n",
    "\n",
    "        # Load image\n",
    "        image = np.array(Image.open(image_path))\n",
    "        assert image.shape == (512, 512, 3), f\"Image {image_path} has wrong shape {image.shape}\"\n",
    "\n",
    "        # Load SA\n",
    "        sa = np.load(sa_path)\n",
    "        assert sa.shape == (64, 64), f\"SA {sa_path} has wrong shape {sa.shape}\"\n",
    "        sa = interpolate(sa, size=(512, 512))\n",
    "        assert sa.shape == (512, 512), f\"SA {sa_path} has wrong shape {sa.shape}\"\n",
    "\n",
    "\n",
    "        # Reescale self-attention to [min_sa, 1]\n",
    "        sa = (sa - sa.min()) / (sa.max() - sa.min())\n",
    "        sa = sa * (1 - min_sa) + min_sa\n",
    "        assert abs(sa.min()- min_sa) < 0.001, f\"Min sa {sa.min()} is not {min_sa}\"\n",
    "        assert abs(sa.max()- 1) < 0.001, f\"Max sa {sa.max()} is not 1\"\n",
    "\n",
    "\n",
    "        # Load annotation. Convert in binary mask\n",
    "        annotation = np.array(Image.open(annotation_path))\n",
    "        \n",
    "        assert annotation.shape == (512, 512, 3), f\"Annotation {annotation_path} has wrong shape {annotation.shape}\"\n",
    "        annotation = annotation.sum(axis=-1) != 0\n",
    "        assert annotation.shape == (512, 512), f\"Annotation aggregated {annotation_path} has wrong shape {annotation.shape}\"\n",
    "        \n",
    "        # Load heatmap\n",
    "        heatmap = heatmap[-2] # We stored in 0 the background and in 1 the token related to the foreground object\n",
    "        assert heatmap.shape == (64, 64), f\"Heatmap {heatmap_path} has wrong shape {heatmap.shape}\"\n",
    "        heatmap = interpolate(heatmap, size=(512, 512), mode=\"bicubic\")\n",
    "        assert heatmap.shape == (512, 512), f\"Heatmap {heatmap_path} has wrong shape {heatmap.shape}\"\n",
    "        \n",
    "        # Binarize using DAAM procedure + dCRF\n",
    "        heatmap = heatmap / heatmap.max()\n",
    "        if apply_sa:\n",
    "            heatmap = heatmap * sa\n",
    "        mask = heatmap > threshold\n",
    "\n",
    "        if apply_crf:\n",
    "            mask = np.stack([1 - mask, mask], axis=-1).astype(np.float32)\n",
    "            mask = densecrf(image, mask)\n",
    "\n",
    "        i_normal, u_normal, iou_normal = compute_iou(annotation=annotation, mask=mask)\n",
    "        example_result_dict['iou_normal'] = iou_normal\n",
    "        example_result_dict['i_normal'] = i_normal\n",
    "        example_result_dict['u_normal'] = u_normal\n",
    "\n",
    "        # Load mask (optimized)\n",
    "        heatmap_optimized = heatmap_optimized[1] # We stored in 0 the background and in 1 the token related to the foreground object\n",
    "        assert heatmap_optimized.shape == (64, 64), f\"Heatmap {heatmap_optimized_path} has wrong shape {heatmap_optimized.shape}\"\n",
    "        heatmap_optimized = interpolate(heatmap_optimized, size=(512, 512), mode=\"bilinear\")\n",
    "        assert heatmap_optimized.shape == (512, 512), f\"Heatmap {heatmap_optimized_path} has wrong shape {heatmap_optimized.shape}\"\n",
    "        \n",
    "        # Rescale sa\n",
    "        sa = (sa - sa.min()) / (sa.max() - sa.min())\n",
    "        sa = sa * (1 - min_sa_optimized) + min_sa_optimized\n",
    "        assert abs(sa.min()- min_sa_optimized) < 0.001, f\"Min sa {sa.min()} is not {min_sa}\"\n",
    "        assert abs(sa.max()- 1) < 0.001, f\"Max sa {sa.max()} is not 1\"\n",
    "\n",
    "        # Binarize using DAAM procedure + dCRF\n",
    "        heatmap_optimized = heatmap_optimized / heatmap_optimized.max()\n",
    "        if apply_sa:\n",
    "            heatmap_optimized = heatmap_optimized * sa\n",
    "        mask_optimized = heatmap_optimized > threshold_optimized\n",
    "        if apply_crf:\n",
    "            mask_optimized = np.stack([1 - mask_optimized, mask_optimized], axis=-1).astype(np.float32)\n",
    "            mask_optimized = densecrf(image, mask_optimized)\n",
    "\n",
    "        i_optimized, u_optimized, iou_optimized = compute_iou(annotation=annotation, mask=mask_optimized)\n",
    "        example_result_dict['iou_optimized'] = iou_optimized\n",
    "        example_result_dict['i_optimized'] = i_optimized\n",
    "        example_result_dict['u_optimized'] = u_optimized\n",
    "\n",
    "        results.append(example_result_dict)    \n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['experiment'] = \"coco-cap - ovam\"\n",
    "    #df_results.to_csv(dataset_path / f'ovam_coco_captions_{suffix_crf}results.csv', index=False)\n",
    "\n",
    "    # All results (included and not included)\n",
    "    assert dataset_path.name == 'coco_cap', f\"Dataset path {dataset_path} is not coco_captions\"\n",
    "\n",
    "    # Aggregated results by class\n",
    "    df_classes = df_results.groupby(['classname', 'model']).aggregate({'i_normal': 'sum', 'u_normal': 'sum', 'i_optimized': 'sum', 'u_optimized': 'sum'}).reset_index()\n",
    "    df_classes['iou_normal'] = df_classes['i_normal'] / df_classes['u_normal']\n",
    "    df_classes['iou_optimized'] = df_classes['i_optimized'] / df_classes['u_optimized']\n",
    "    df_classes['experiment'] = \"voc-sim - ovam\"\n",
    "    df_classes = df_classes.sort_values('classname').reset_index(drop=True)\n",
    "    #df_classes.to_csv(dataset_path / f'ovam-cap_class_results_all{suffix_crf}.csv', index=False)\n",
    "\n",
    "    df_classes_display = df_classes[[\"classname\", 'iou_normal', 'iou_optimized']].copy()\n",
    "    df_classes_display['iou_normal'] = (100*df_classes_display['iou_normal']).round(1)\n",
    "    df_classes_display['iou_optimized'] = (100*df_classes_display['iou_optimized']).round(1)\n",
    "    #df_classes_display.T.to_excel(dataset_path / f'ovam-cap_class_results_all{suffix_crf}.xlsx', index=False)\n",
    "\n",
    "    # display(df_classes_display.T)\n",
    "\n",
    "\n",
    "    # Aggregate overall results\n",
    "    df_overall = df_classes.groupby('model').aggregate({'i_normal': 'sum', 'u_normal': 'sum', 'i_optimized': 'sum', 'u_optimized': 'sum',\n",
    "                                        'iou_normal': 'mean', 'iou_optimized': 'mean'}).reset_index()\n",
    "\n",
    "    df_overall.rename(columns={'iou_normal': 'miou_normal', 'iou_optimized': 'miou_optimized'}, inplace=True)\n",
    "    df_overall['iou_overall_normal'] = df_overall['i_normal'] / df_overall['u_normal']\n",
    "    df_overall['iou_overall_optimized'] = df_overall['i_optimized'] / df_overall['u_optimized']\n",
    "    df_overall['experiment'] = \"coco-cap - grounded diffusion\"\n",
    "    #df_overall.to_csv(dataset_path / f'ovam-cap_overall_results_all{suffix_crf}.csv', index=False)\n",
    "    df_overall_display = df_overall[['iou_overall_normal', 'miou_normal', 'iou_overall_optimized', 'miou_optimized']].copy()\n",
    "    df_overall_display = (100*df_overall_display).round(1)\n",
    "    df_overall_display[\"dcrf\"] = apply_crf\n",
    "    df_overall_display[\"sa\"] = apply_sa\n",
    "    display(df_overall_display[[\"miou_normal\",\"iou_overall_normal\", \"miou_optimized\",  \"iou_overall_optimized\", \"sa\", \"dcrf\"]])\n",
    "\n",
    "    re_results.append(df_overall_display)\n",
    "\n",
    "df_reresults = pd.concat(re_results)\n",
    "df_reresults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert dataset_path.name == 'coco_cap', f\"Dataset path {dataset_path} is not coco_captions\"\n",
    "\n",
    "for included in [True, False]:\n",
    "    included_str = 'included' if included else 'non_included'\n",
    "    print(included_str)\n",
    "    # Aggregated results by class\n",
    "    df_classes = df_results.query(\"word_included==@included\").groupby(['classname', 'model']).aggregate({'i_normal': 'sum', 'u_normal': 'sum', 'i_optimized': 'sum', 'u_optimized': 'sum'}).reset_index()\n",
    "    df_classes['iou_normal'] = df_classes['i_normal'] / df_classes['u_normal']\n",
    "    df_classes['iou_optimized'] = df_classes['i_optimized'] / df_classes['u_optimized']\n",
    "    df_classes['experiment'] = \"voc-sim - grounded diffusion\"\n",
    "    df_classes = df_classes.sort_values('classname').reset_index(drop=True)\n",
    "    #df_classes.to_csv(dataset_path / 'grounded_diffusion_coco-cap_class_results_all.csv', index=False)\n",
    "\n",
    "    df_classes_display = df_classes[[\"classname\", 'iou_normal', 'iou_optimized']].copy()\n",
    "    df_classes_display['iou_normal'] = (100*df_classes_display['iou_normal']).round(1)\n",
    "    df_classes_display['iou_optimized'] = (100*df_classes_display['iou_optimized']).round(1)\n",
    "    df_classes_display.T.to_excel(dataset_path / f'ovam-cap_class_results_{included_str}_{suffix_crf}.xlsx', index=False)\n",
    "\n",
    "    display(df_classes_display.T)\n",
    "\n",
    "    # Aggregate overall results\n",
    "    df_overall = df_classes.groupby('model').aggregate({'i_normal': 'sum', 'u_normal': 'sum', 'i_optimized': 'sum', 'u_optimized': 'sum',\n",
    "                                        'iou_normal': 'mean', 'iou_optimized': 'mean'}).reset_index()\n",
    "\n",
    "    df_overall.rename(columns={'iou_normal': 'miou_normal', 'iou_optimized': 'miou_optimized'}, inplace=True)\n",
    "    df_overall['iou_overall_normal'] = df_overall['i_normal'] / df_overall['u_normal']\n",
    "    df_overall['iou_overall_optimized'] = df_overall['i_optimized'] / df_overall['u_optimized']\n",
    "    df_overall['experiment'] = \"coco-cap - grounded diffusion\"\n",
    "    df_overall.to_csv(dataset_path / f'ovam_coco-cap_overall_results_{included_str}_{suffix_crf}.csv', index=False)\n",
    "\n",
    "    df_overall_display = df_overall[['miou_normal', 'iou_overall_normal',  'miou_optimized', 'iou_overall_optimized']].copy()\n",
    "    df_overall_display = (100*df_overall_display).round(1)\n",
    "    display(df_overall_display)\n",
    "\n",
    "df_reresults = pd.concat(re_results)\n",
    "df_reresults\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diffusion_21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
